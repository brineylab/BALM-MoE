{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907c0a96-0fa5-486c-ad3b-6da1ea3bf0b5",
   "metadata": {},
   "source": [
    "# Training BALM-MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8139b-0fea-45be-8eb7-3462aa4968fd",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4676a8c2-28bd-4c78-8cf0-091b16879725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass, field, fields\n",
    "from enum import Enum\n",
    "\n",
    "class StrEnum(str, Enum):\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "from typing import Optional, Tuple, List, Dict, Any, Iterable, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd08a74-e838-4a14-95b9-35e42cdbf9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from balm.config import BalmConfig, BalmMoEConfig\n",
    "from balm.data import load_dataset, DataCollator\n",
    "from balm.models import (\n",
    "    BalmForMaskedLM,\n",
    "    BalmModel,\n",
    "    BalmMoEForMaskedLM,\n",
    ")\n",
    "from balm.tokenizer import Tokenizer\n",
    "from balm.train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6313dec-7c21-404e-a4d2-034cedfa34ab",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f88e77-3193-4210-baa8-34303732ce1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./balm/vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668ac44-b280-417e-aceb-24692fa04458",
   "metadata": {},
   "source": [
    "## Load and Clean Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccf8e1b-146d-48b3-8610-fe6bd21c2fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/training-data/jaffe_lc-coherence/paired/LC-coherence_90-5-5/train.txt\",\n",
    "    \"eval\": \"/training-data/jaffe_lc-coherence/paired/LC-coherence_90-5-5/eval.txt\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efb9e95-cf87-4612-914f-e7af93cd562a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict\n",
       "-----------\n",
       "  train\n",
       "    num_rows: 1202270\n",
       "    columns: ['text']\n",
       "  eval\n",
       "    num_rows: 66792\n",
       "    columns: ['text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26be838-19de-4293-85a9-249aefa3fbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_name = \"balmMoE_expertchoice_1shared_altern_052924\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c7dd5-259e-48e2-b77c-dee62db67c11",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65590e71-2f8f-45f9-8432-070ff1ce7c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d668cae9b0045c09862571cc3ad5f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1202270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87847b12a1b447af8dc26ba736b914d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/66792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "     remove_columns=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a476921-a338-48ad-afe9-8afedfd81d3b",
   "metadata": {},
   "source": [
    "## Load Collator and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae8787b-d817-4ccf-a054-f3a91e1e2287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667ec9be-ba81-4e4b-af56-2aca9a655755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"eval\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ad71b-6262-45e3-910d-0b883e03ece4",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af0b26b-5406-41db-b5ce-55bb31e5acb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = BalmMoEConfig(\n",
    "    expert_choice_router=True,\n",
    "    embed_dim=960,\n",
    "    ffn_dim=3840,\n",
    "    num_layers=6,\n",
    "    num_experts=16,\n",
    "    num_heads=20,\n",
    "    num_shared_experts=1,\n",
    "    alternate_sparsity=True,\n",
    "    expert_capacity=128,\n",
    "    router_z_loss_coef=0.01,\n",
    "    router_aux_loss_coef=0.01,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033bb7c5-2183-4081-b66e-95b5fc8696de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BalmMoEForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f0d195-9789-47d4-87c7-cf31a0ee166a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 305.22M\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31492e89-18a3-47a7-9f3a-9b31865b22df",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ddd1cf-4756-4f19-a23f-399e989703d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    output_dir=\"./training_runs/balmMoE_expertchoice_1shared_altern_052924\",\n",
    "    #epochs=1,\n",
    "    max_steps=500000,\n",
    "    logging_steps=100,\n",
    "    eval_steps=25000,\n",
    "    warmup_steps=30000,\n",
    "    learning_rate=16e-4,\n",
    "    # save_steps=15,\n",
    "    per_device_train_batch_size=32,\n",
    "    # use_cpu=True,\n",
    "    use_wandb=True,\n",
    "    wandb_project=\"balm_moe\",\n",
    "    # wandb_entity=\"bryanbriney\",\n",
    "    run_name=\"balmMoE_expertchoiceBig_1shared_altern_052924\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4980e59-6f46-4acd-8429-2ce2bee52766",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrineylab\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/shared/simone/BALM_development_62024/training_runs/balmMoE_expertchoice_1shared_altern_300M_72224/log/wandb/run-20240722_212346-qvaotzlq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brineylab/balm_moe/runs/qvaotzlq' target=\"_blank\">balmMoE_expertchoiceBig_1shared_altern_300M_72224</a></strong> to <a href='https://wandb.ai/brineylab/balm_moe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brineylab/balm_moe' target=\"_blank\">https://wandb.ai/brineylab/balm_moe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brineylab/balm_moe/runs/qvaotzlq' target=\"_blank\">https://wandb.ai/brineylab/balm_moe/runs/qvaotzlq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883d2decb6184be7a7d3d8aea3db3662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500000 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shared/simone/BALM_development_62024/balm/data.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v) for k, v in examples.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100   | loss: 2.7781 | MLM loss: 2.7041 | router z-loss: 0.0739 | lr: 0.000005\n",
      "step 200   | loss: 2.4280 | MLM loss: 2.3845 | router z-loss: 0.0435 | lr: 0.000011\n",
      "step 300   | loss: 2.1152 | MLM loss: 2.0943 | router z-loss: 0.0208 | lr: 0.000016\n",
      "step 400   | loss: 1.9178 | MLM loss: 1.9097 | router z-loss: 0.0080 | lr: 0.000021\n",
      "step 500   | loss: 1.6520 | MLM loss: 1.6475 | router z-loss: 0.0044 | lr: 0.000027\n",
      "step 600   | loss: 1.3631 | MLM loss: 1.3591 | router z-loss: 0.0040 | lr: 0.000032\n",
      "step 700   | loss: 1.1150 | MLM loss: 1.1109 | router z-loss: 0.0041 | lr: 0.000037\n",
      "step 800   | loss: 0.9257 | MLM loss: 0.9211 | router z-loss: 0.0046 | lr: 0.000043\n",
      "step 900   | loss: 0.7853 | MLM loss: 0.7810 | router z-loss: 0.0043 | lr: 0.000048\n",
      "step 1000  | loss: 0.7495 | MLM loss: 0.7449 | router z-loss: 0.0046 | lr: 0.000053\n",
      "step 1100  | loss: 0.6385 | MLM loss: 0.6345 | router z-loss: 0.0040 | lr: 0.000059\n",
      "step 1200  | loss: 0.5610 | MLM loss: 0.5572 | router z-loss: 0.0039 | lr: 0.000064\n",
      "step 1300  | loss: 0.4532 | MLM loss: 0.4497 | router z-loss: 0.0035 | lr: 0.000069\n",
      "step 1400  | loss: 0.5046 | MLM loss: 0.5016 | router z-loss: 0.0030 | lr: 0.000075\n",
      "step 1500  | loss: 0.4530 | MLM loss: 0.4500 | router z-loss: 0.0030 | lr: 0.000080\n",
      "step 1600  | loss: 0.4069 | MLM loss: 0.4042 | router z-loss: 0.0026 | lr: 0.000085\n",
      "step 1700  | loss: 0.3760 | MLM loss: 0.3737 | router z-loss: 0.0023 | lr: 0.000091\n",
      "step 1800  | loss: 0.3559 | MLM loss: 0.3537 | router z-loss: 0.0022 | lr: 0.000096\n",
      "step 1900  | loss: 0.3836 | MLM loss: 0.3816 | router z-loss: 0.0020 | lr: 0.000101\n",
      "step 2000  | loss: 0.3764 | MLM loss: 0.3748 | router z-loss: 0.0016 | lr: 0.000107\n",
      "step 2100  | loss: 0.3182 | MLM loss: 0.3165 | router z-loss: 0.0017 | lr: 0.000112\n",
      "step 2200  | loss: 0.3404 | MLM loss: 0.3390 | router z-loss: 0.0014 | lr: 0.000117\n",
      "step 2300  | loss: 0.3326 | MLM loss: 0.3313 | router z-loss: 0.0013 | lr: 0.000123\n",
      "step 2400  | loss: 0.3355 | MLM loss: 0.3344 | router z-loss: 0.0011 | lr: 0.000128\n",
      "step 2500  | loss: 0.3265 | MLM loss: 0.3254 | router z-loss: 0.0011 | lr: 0.000133\n",
      "step 2600  | loss: 0.3228 | MLM loss: 0.3218 | router z-loss: 0.0010 | lr: 0.000139\n",
      "step 2700  | loss: 0.2906 | MLM loss: 0.2895 | router z-loss: 0.0011 | lr: 0.000144\n",
      "step 2800  | loss: 0.2562 | MLM loss: 0.2552 | router z-loss: 0.0010 | lr: 0.000149\n",
      "step 2900  | loss: 0.2880 | MLM loss: 0.2871 | router z-loss: 0.0009 | lr: 0.000155\n",
      "step 3000  | loss: 0.2876 | MLM loss: 0.2868 | router z-loss: 0.0008 | lr: 0.000160\n",
      "step 3100  | loss: 0.2952 | MLM loss: 0.2945 | router z-loss: 0.0007 | lr: 0.000165\n",
      "step 3200  | loss: 0.2945 | MLM loss: 0.2939 | router z-loss: 0.0007 | lr: 0.000171\n",
      "step 3300  | loss: 0.2990 | MLM loss: 0.2983 | router z-loss: 0.0006 | lr: 0.000176\n",
      "step 3400  | loss: 0.2638 | MLM loss: 0.2632 | router z-loss: 0.0006 | lr: 0.000181\n",
      "step 3500  | loss: 0.2533 | MLM loss: 0.2527 | router z-loss: 0.0006 | lr: 0.000187\n",
      "step 3600  | loss: 0.2892 | MLM loss: 0.2886 | router z-loss: 0.0006 | lr: 0.000192\n",
      "step 3700  | loss: 0.3351 | MLM loss: 0.3346 | router z-loss: 0.0005 | lr: 0.000197\n",
      "step 3800  | loss: 0.2387 | MLM loss: 0.2382 | router z-loss: 0.0005 | lr: 0.000203\n",
      "step 3900  | loss: 0.2463 | MLM loss: 0.2458 | router z-loss: 0.0005 | lr: 0.000208\n",
      "step 4000  | loss: 0.2466 | MLM loss: 0.2461 | router z-loss: 0.0005 | lr: 0.000213\n",
      "step 4100  | loss: 0.2856 | MLM loss: 0.2852 | router z-loss: 0.0004 | lr: 0.000219\n",
      "step 4200  | loss: 0.2645 | MLM loss: 0.2641 | router z-loss: 0.0004 | lr: 0.000224\n",
      "step 4300  | loss: 0.2473 | MLM loss: 0.2469 | router z-loss: 0.0004 | lr: 0.000229\n",
      "step 4400  | loss: 0.2916 | MLM loss: 0.2913 | router z-loss: 0.0004 | lr: 0.000235\n",
      "step 4500  | loss: 0.2397 | MLM loss: 0.2393 | router z-loss: 0.0004 | lr: 0.000240\n",
      "step 4600  | loss: 0.2475 | MLM loss: 0.2471 | router z-loss: 0.0003 | lr: 0.000245\n",
      "step 4700  | loss: 0.2215 | MLM loss: 0.2212 | router z-loss: 0.0003 | lr: 0.000251\n",
      "step 4800  | loss: 0.2497 | MLM loss: 0.2493 | router z-loss: 0.0004 | lr: 0.000256\n",
      "step 4900  | loss: 0.2747 | MLM loss: 0.2744 | router z-loss: 0.0003 | lr: 0.000261\n",
      "step 5000  | loss: 0.2678 | MLM loss: 0.2675 | router z-loss: 0.0003 | lr: 0.000267\n",
      "step 5100  | loss: 0.2724 | MLM loss: 0.2721 | router z-loss: 0.0003 | lr: 0.000272\n",
      "step 5200  | loss: 0.3126 | MLM loss: 0.3124 | router z-loss: 0.0003 | lr: 0.000277\n",
      "step 5300  | loss: 0.2630 | MLM loss: 0.2627 | router z-loss: 0.0003 | lr: 0.000283\n",
      "step 5400  | loss: 0.2685 | MLM loss: 0.2683 | router z-loss: 0.0002 | lr: 0.000288\n",
      "step 5500  | loss: 0.2374 | MLM loss: 0.2371 | router z-loss: 0.0002 | lr: 0.000293\n",
      "step 5600  | loss: 0.2535 | MLM loss: 0.2533 | router z-loss: 0.0003 | lr: 0.000299\n",
      "step 5700  | loss: 0.2206 | MLM loss: 0.2204 | router z-loss: 0.0002 | lr: 0.000304\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "#wandb.login()\n",
    "#wandb.init(project = 'balm_moe', name='balmMoE_expertchoice_1shared_0altern_052924')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b37731-4190-42c7-bf8f-4becc4b43729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "torch.save(model.state_dict(), f'./models/{run_name}_{model_size/1e6:.2f}Mp.pth')\n",
    "#trainer.save_model(f'./models/{run_name}_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3db8814-2261-4113-9485-e115b8371e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
